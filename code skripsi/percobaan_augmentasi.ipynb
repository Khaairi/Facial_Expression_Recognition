{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f52c85-4ef4-4943-a521-ff740da659ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu124'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85719059-ed85-4e86-a40a-e26217642235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b309cc-ab32-4ea3-8c91-f1821515f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from mlxtend.plotting import plot_confusion_matrix\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d82b357-c8e6-439f-a137-ad758ecb7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe35ee-b631-403e-ae59-b7016ed13458",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bdddb0d-9e30-4147-938e-a8109e906f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "        # Ekstrak label dan piksel\n",
    "        self.labels = self.dataframe['emotion'].values\n",
    "        self.pixels = self.dataframe['pixels'].apply(self.string_to_image).values\n",
    "\n",
    "    def string_to_image(self, pixels_string):\n",
    "        # Konversi string piksel menjadi numpy array dan reshape ke 48x48\n",
    "        pixels = np.array(pixels_string.split(), dtype='float32')\n",
    "        image = pixels.reshape(48, 48)\n",
    "        image = np.expand_dims(image, axis=-1)  # Tambahkan channel dimensi\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixels[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.fromarray(image.squeeze().astype('uint8'), mode='L')\n",
    "\n",
    "        # Jika ada transformasi, terapkan ke image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e547083-87ac-403c-8652-c466f6382284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedFERDataset(FERDataset):\n",
    "    def __init__(self, dataframe, classes_to_augment, transform=None, augment_transform=None, target_count=None):\n",
    "        super().__init__(dataframe, transform)\n",
    "        self.augment_transform = augment_transform\n",
    "        self.augmented_data = []  # Simpan semua hasil augmentasi\n",
    "        \n",
    "        for class_to_augment in classes_to_augment:\n",
    "            self.augment_class(class_to_augment, target_count)\n",
    "\n",
    "    def augment_class(self, class_to_augment, target_count):\n",
    "        # Filter sampel minoritas\n",
    "        minority_indices = np.where(self.labels == class_to_augment)[0]\n",
    "        minority_samples = self.pixels[minority_indices]\n",
    "\n",
    "        # Pastikan ada data untuk augmentasi\n",
    "        if len(minority_samples) == 0:\n",
    "            print(f\"Tidak ada sampel untuk kelas {class_to_augment}, lewati augmentasi.\")\n",
    "            return\n",
    "\n",
    "        # Target augmentasi, minimal dua kali data asli jika target_count tidak diberikan\n",
    "        target_count = target_count if target_count else len(minority_samples) * 2\n",
    "        augmented_images = self.augment_minority_data(minority_samples, target_count)\n",
    "\n",
    "        # Tambahkan hasil augmentasi ke data\n",
    "        self.augmented_data.extend([(img, class_to_augment) for img in augmented_images])\n",
    "\n",
    "    def augment_minority_data(self, minority_samples, target_count):\n",
    "        augmented_images = []\n",
    "        while len(augmented_images) < target_count - len(minority_samples):\n",
    "            idx = np.random.choice(len(minority_samples))\n",
    "            image = Image.fromarray(minority_samples[idx].squeeze().astype('uint8'), mode='L')\n",
    "            augmented_image = self.augment_transform(image)\n",
    "            augmented_images.append(augmented_image)\n",
    "        return augmented_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Jika idx di luar data asli, ambil dari augmented_data\n",
    "        if idx < len(self.pixels):\n",
    "            image, label = super().__getitem__(idx)\n",
    "        else:\n",
    "            idx -= len(self.pixels)\n",
    "            image, label = self.augmented_data[idx]\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pixels) + len(self.augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fd05f22-3a1d-495e-ab37-5912522b0f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforms: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5], std=[0.5])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Create transform pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])           \n",
    "print(f\"transforms: {manual_transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fbcb8a-9f34-4367-a1fe-a5ad03453073",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip horizontally\n",
    "    transforms.RandomRotation(10),     # Randomly rotate by +/- 10 degrees\n",
    "    transforms.RandomResizedCrop(224), # Randomly crop and resize to 224x224\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust color\n",
    "    # transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    # transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb51cb0-db73-45d5-b795-d7bbe982676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/fer2013_clean.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e14dbbfc-b85a-49e4-aacf-48aec10ad49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = pd.read_csv('../Data/fer_train.csv')\n",
    "data_val = pd.read_csv('../Data/fer_val.csv')\n",
    "data_test = pd.read_csv('../Data/fer_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525f87c-e3d3-4893-9194-b81b2a4b80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pertama, pisahkan data train (90%) dan validation (10%)\n",
    "data_train, data_test = train_test_split(data, test_size=0.1, stratify=data['emotion'], random_state=123)\n",
    "data_train, data_val = train_test_split(data_train, test_size=0.1, stratify=data_train['emotion'], random_state=123)\n",
    "\n",
    "# Cek ukuran masing-masing set untuk memastikan proporsi\n",
    "print(f'Train set size: {len(data_train)}')\n",
    "print(f'Validation set size: {len(data_val)}')\n",
    "print(f'Test set size: {len(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d2b04-f7d5-47ba-a8c4-806b9ec645c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung jumlah sampel per kelas\n",
    "class_counts = data_train['emotion'].value_counts()\n",
    "print(\"Distribusi kelas:\\n\", class_counts)\n",
    "\n",
    "# Menentukan jumlah kelas mayoritas\n",
    "majority_class_count = class_counts.max()\n",
    "\n",
    "# Menentukan kelas minoritas (yang membutuhkan augmentasi)\n",
    "classes_to_augment = class_counts[class_counts < majority_class_count].index.tolist()\n",
    "\n",
    "print(f\"Kelas yang perlu di-augmentasi: {classes_to_augment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112b864-08d8-42cb-a285-994f930f4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AugmentedFERDataset(\n",
    "    data_train,\n",
    "    classes_to_augment=classes_to_augment,\n",
    "    transform=manual_transforms,\n",
    "    augment_transform=augmentations,\n",
    "    target_count=majority_class_count\n",
    ")\n",
    "val_dataset = FERDataset(data_val, transform=manual_transforms)\n",
    "test_dataset = FERDataset(data_test, transform=manual_transforms)\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e29afba4-2433-4e7d-bcae-f14d5879daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FERDataset(data_train, transform=manual_transforms)\n",
    "val_dataset = FERDataset(data_val, transform=manual_transforms)\n",
    "test_dataset = FERDataset(data_test, transform=manual_transforms)\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f041997-38dc-48c9-a53d-94e8f07d3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total data setelah augmentasi: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6f386-740d-4f3f-b78f-a0aef62e1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for idx in range(len(train_dataset)):\n",
    "    image, label = train_dataset[idx]\n",
    "    # Convert image back to a flattened string (if needed)\n",
    "    image_array = np.array(image).flatten()\n",
    "    image_str = ' '.join(map(str, image_array))\n",
    "    data.append([image_str, label])\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = ['pixels', 'emotion']\n",
    "augmented_df = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec020374-fc9f-4eb2-ab3e-01812bc124d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18801b11-0a9d-4d05-82b2-2677f760c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e6183-55c5-4bd9-a417-7459e1fd1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.to_csv('../Data/fer_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d29671-8d28-4f6f-bb75-db6c8bf6516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val.to_csv('../Data/fer_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ecaaea-d946-4595-822d-3db1f3123856",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_csv('../Data/fer_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e5ea5-2638-46dd-bccc-511c9fd06446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dapatkan labels dari train_dataset\n",
    "labels = [label for _, label in train_dataset]\n",
    "\n",
    "# Hitung distribusi kelas\n",
    "label_counts = pd.Series(labels).value_counts()\n",
    "\n",
    "# Ubah menjadi DataFrame dengan nama kolom yang sesuai\n",
    "label_counts_df = label_counts.sort_index().reset_index()\n",
    "label_counts_df.columns = ['emotion', 'number']\n",
    "\n",
    "# Menampilkan distribusi kelas dalam DataFrame\n",
    "label_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c40770a-6a57-48ca-a10f-8a6c3ff485f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {'Angry': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3, 'Sad': 4, 'Surprise': 5, 'Neutral': 6}\n",
      "id2label: {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n"
     ]
    }
   ],
   "source": [
    "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Mapping dari label ke ID (label2id)\n",
    "label2id = {c: idx for idx, c in enumerate(labels)}\n",
    "\n",
    "# Mapping dari ID ke label (id2label)\n",
    "id2label = {idx: c for idx, c in enumerate(labels)}\n",
    "\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27139b4d-f206-4e4f-82e6-6c26b311b7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ambil satu contoh dari train_dataset\n",
    "image, label = test_dataset[0]  # Index pertama dataset\n",
    "\n",
    "# Jika transform menghasilkan tensor, konversi ke format numpy\n",
    "if isinstance(image, torch.Tensor):\n",
    "    image = image.permute(1, 2, 0).numpy()  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "# Plot gambar\n",
    "plt.imshow(image, cmap='gray')  # Gunakan cmap='gray' jika gambar grayscale\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')  # Hilangkan sumbu\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a0a00-f3ca-46e1-8080-faf95a66d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_dataset[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f09b62-4109-49eb-ae60-3ce50168f049",
   "metadata": {},
   "source": [
    "## Build Vision Transformer (ViT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f5c19-e39f-478b-904a-4070f72d2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7697f-f6eb-4ea2-ab66-bcfdacdf4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels:int=1,\n",
    "                 patch_size:int=16,\n",
    "                 embedding_dim:int=768\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "    def forward(self, x):\n",
    "        image_resolution = x.shape[-1]\n",
    "        \n",
    "        x_patched = self.patcher(x) # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x_flattened = x_patched.flatten(2) # (n_samples, embed_dim, n_patches)\n",
    "        x = x_flattened.transpose(1, 2) # (n_samples, n_patches, embed_dim)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc468a7-138e-48ca-a495-a91e0988bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True)\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.multihead_attn(query=x,\n",
    "                                             key=x,\n",
    "                                             value=x,\n",
    "                                             need_weights=False)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cdbef5-da91-45c7-8847-9de18bcd77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 mlp_size:int=3072,\n",
    "                 dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size,\n",
    "                      out_features=embedding_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8d7b1-8603-4ade-8bed-32444a8c942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 mlp_size:int=3072,\n",
    "                 mlp_dropout:float=0.,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.msa_block(self.layer_norm1(x)) + x \n",
    "        \n",
    "        x = self.mlp_block(self.layer_norm2(x)) + x \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe073e-b7c5-44c3-a123-d89a2152a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size:int=224,\n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=16,\n",
    "                 num_transformer_layers:int=12,\n",
    "                 embedding_dim:int=768,\n",
    "                 mlp_size:int=3072,\n",
    "                 num_heads:int=12,\n",
    "                 attn_dropout:float=0.,\n",
    "                 mlp_dropout:float=0.,\n",
    "                 embedding_dropout:float=0.,\n",
    "                 num_classes:int=1000):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "                 \n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "        \n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6),\n",
    "            nn.Linear(in_features=embedding_dim, \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "\n",
    "        x = self.patch_embedding(pixel_values) # (n_samples, num_patches, embed_dim)\n",
    "        \n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) # (n_samples, 1, embed_dim)\n",
    "\n",
    "        x = torch.cat((class_token, x), dim=1) # (n_samples, 1 + n_patches, embed_dim)\n",
    "\n",
    "        x = self.position_embedding + x # add position embed\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        logits = self.classifier(x[:, 0])\n",
    "\n",
    "        # Jika labels diberikan, hitung loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "    \n",
    "        return (loss, logits) if loss is not None else logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccbb722-51b9-46af-90e2-4b80265eb878",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b28361-dae6-43fa-b846-69caa47259a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(num_classes=len(class_names), in_channels=1, patch_size=32, num_heads=8, embedding_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2344b3-32fa-4f94-9c56-13b3ac96ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    model=model,\n",
    "    input_size=(1, 1, 224, 224),  # (batch_size, in_channels, img_size, img_size)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=15,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ece644-8389-406b-b8e9-802ac5a1f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_CLASSES = 7\n",
    "SEED = 123\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Define path\n",
    "SAVE_PATH = \"../Hasil Eksperimen\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Initialize lists to store training and validation metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Initialize the best metric for model saving\n",
    "best_val_accuracy = -float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{train_loss / (batch_idx + 1):.4f}\",\n",
    "            \"Acc\": f\"{correct / total:.4f}\"\n",
    "        })\n",
    "\n",
    "    # Calculate training accuracy and loss\n",
    "    train_accuracy = correct / total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Print training summary\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_targets = []\n",
    "    all_predicted = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        pbar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS} (Validation)\")\n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Update statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # Collect all targets and predictions for F1-score\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{val_loss / (batch_idx + 1):.4f}\",\n",
    "                \"Acc\": f\"{val_correct / val_total:.4f}\"\n",
    "            })\n",
    "\n",
    "    # Calculate validation accuracy, loss, and F1-score\n",
    "    val_accuracy = val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_f1 = f1_score(all_targets, all_predicted, average=\"weighted\")\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print validation summary\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_accuracy:.4f}, \"\n",
    "          f\"Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Step the learning rate scheduler based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Print the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        model_path = os.path.join(SAVE_PATH, \"aug_off_model.pt\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Best model saved at {model_path} with val accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "    # Save loss and accuracy plots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker='o')\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(SAVE_PATH, \"aug_off_loss.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Training Accuracy\", marker='o')\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label=\"Validation Accuracy\", marker='o')\n",
    "    plt.title(\"Accuracy per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    accuracy_plot_path = os.path.join(SAVE_PATH, \"aug_off_acc.png\")\n",
    "    plt.savefig(accuracy_plot_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b0754-5394-47cb-a6ea-fb9ac2ca2ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
