{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f52c85-4ef4-4943-a521-ff740da659ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85719059-ed85-4e86-a40a-e26217642235",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b309cc-ab32-4ea3-8c91-f1821515f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77039c05-9f1a-430a-89b0-b22f0d4de2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe35ee-b631-403e-ae59-b7016ed13458",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdddb0d-9e30-4147-938e-a8109e906f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "        # Ekstrak label dan piksel\n",
    "        self.labels = self.dataframe['emotion'].values\n",
    "        self.pixels = self.dataframe['pixels'].apply(self.string_to_image).values\n",
    "\n",
    "    def string_to_image(self, pixels_string):\n",
    "        # Konversi string piksel menjadi numpy array dan reshape ke 48x48\n",
    "        pixels = np.array(pixels_string.split(), dtype='float32')\n",
    "        image = pixels.reshape(48, 48)\n",
    "        image = np.expand_dims(image, axis=-1)  # Tambahkan channel dimensi\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixels[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.fromarray(image.squeeze().astype('uint8'), mode='L')\n",
    "\n",
    "        # Jika ada transformasi, terapkan ke image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd05f22-3a1d-495e-ab37-5912522b0f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Create transform pipeline manually\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip horizontally\n",
    "    transforms.RandomRotation(10),     # Randomly rotate by 10 degrees\n",
    "    transforms.RandomResizedCrop(\n",
    "        size=IMG_SIZE,  # Output size\n",
    "        scale=(0.8, 1.0)  # Range of the random crop size relative to the input size\n",
    "    ),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "]) \n",
    "\n",
    "# Create transform pipeline manually\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    " \n",
    "print(f\"train transforms: {train_transforms}\")\n",
    "print(f\"test transforms: {test_transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb51cb0-db73-45d5-b795-d7bbe982676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/fer2013_clean.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525f87c-e3d3-4893-9194-b81b2a4b80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pertama, pisahkan data train (90%) dan validation (10%)\n",
    "data_train, data_test = train_test_split(data, test_size=0.1, stratify=data['emotion'], random_state=123)\n",
    "data_train, data_val = train_test_split(data_train, test_size=0.1, stratify=data_train['emotion'], random_state=123)\n",
    "\n",
    "# Cek ukuran masing-masing set untuk memastikan proporsi\n",
    "print(f'Train set size: {len(data_train)}')\n",
    "print(f'Validation set size: {len(data_val)}')\n",
    "print(f'Test set size: {len(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112b864-08d8-42cb-a285-994f930f4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FERDataset(data_train, transform=train_transforms)\n",
    "val_dataset = FERDataset(data_val, transform=test_transforms)\n",
    "test_dataset = FERDataset(data_test, transform=test_transforms)\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40770a-6a57-48ca-a10f-8a6c3ff485f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Mapping dari label ke ID (label2id)\n",
    "label2id = {c: idx for idx, c in enumerate(labels)}\n",
    "\n",
    "# Mapping dari ID ke label (id2label)\n",
    "id2label = {idx: c for idx, c in enumerate(labels)}\n",
    "\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27139b4d-f206-4e4f-82e6-6c26b311b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mean and std used for normalization\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "# Denormalize function\n",
    "def denormalize(tensor, mean, std):\n",
    "    # Clone the tensor to avoid modifying the original\n",
    "    tensor = tensor.clone()\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Reverse the normalization: (t * s) + m\n",
    "    return tensor\n",
    "\n",
    "# Ambil satu contoh dari train_dataset\n",
    "image, label = train_dataset[0]  # Index pertama dataset\n",
    "\n",
    "# Denormalize the image\n",
    "image = denormalize(image, mean, std)\n",
    "\n",
    "# Jika transform menghasilkan tensor, konversi ke format numpy\n",
    "if isinstance(image, torch.Tensor):\n",
    "    image = image.permute(1, 2, 0).numpy()  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "# Clip the values to the valid range [0, 1]\n",
    "image = image.clip(0, 1)\n",
    "\n",
    "# Plot gambar\n",
    "plt.imshow(image, cmap='gray')  # Gunakan cmap='gray' jika gambar grayscale\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')  # Hilangkan sumbu\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a0a00-f3ca-46e1-8080-faf95a66d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_dataset[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f09b62-4109-49eb-ae60-3ce50168f049",
   "metadata": {},
   "source": [
    "## Build Vision Transformer (ViT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f5c19-e39f-478b-904a-4070f72d2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc468a7-138e-48ca-a495-a91e0988bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True)\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.multihead_attn(query=x,\n",
    "                                             key=x,\n",
    "                                             value=x,\n",
    "                                             need_weights=False)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cdbef5-da91-45c7-8847-9de18bcd77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 mlp_size:int=3072,\n",
    "                 dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size,\n",
    "                      out_features=embedding_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8d7b1-8603-4ade-8bed-32444a8c942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 mlp_size:int=3072,\n",
    "                 mlp_dropout:float=0.,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.msa_block(self.layer_norm1(x)) + x \n",
    "        \n",
    "        x = self.mlp_block(self.layer_norm2(x)) + x \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe073e-b7c5-44c3-a123-d89a2152a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0., # Dropout for attention projection\n",
    "                 mlp_dropout:float=0., # Dropout for dense/MLP layers \n",
    "                 embedding_dropout:float=0., # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__()\n",
    "         \n",
    "        assert img_size % 32 == 0, f\"Image size must be divisible by 32, image size: {img_size}\"\n",
    "        \n",
    "        self.backbone = efficientnet_b4(weights=EfficientNet_B4_Weights.IMAGENET1K_V1).features\n",
    "\n",
    "        # ResNet50's last conv layer outputs 2048 channels\n",
    "        self.projection = nn.Conv2d(in_channels=1792,  \n",
    "                                    out_channels=embedding_dim,\n",
    "                                    kernel_size=1)\n",
    "                 \n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "\n",
    "        self.num_patches = (img_size // 32) ** 2  # Resnet reduces spatial size by 32x\n",
    "        \n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        \n",
    "        batch_size = pixel_values.shape[0]\n",
    "\n",
    "        # Extract features using MobileNetV3\n",
    "        features = self.backbone(pixel_values)  # Output shape: (batch_size, backbone_out_channels, H', W')\n",
    "        features = self.projection(features)  # Project to embedding_dim: (batch_size, embedding_dim, H', W')\n",
    "\n",
    "        # Flatten the feature maps into a sequence of tokens\n",
    "        features = features.flatten(2).transpose(1, 2)  # Shape: (batch_size, num_patches, embedding_dim)\n",
    "        \n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1)\n",
    "\n",
    "        x = torch.cat((class_token, features), dim=1)  # Shape: (batch_size, num_patches + 1, embedding_dim)\n",
    "\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        logits = self.classifier(x[:, 0])\n",
    "\n",
    "        # Jika labels diberikan, hitung loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "    \n",
    "        return (loss, logits) if loss is not None else logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccbb722-51b9-46af-90e2-4b80265eb878",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b28361-dae6-43fa-b846-69caa47259a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(num_classes=len(class_names), in_channels=3, patch_size=32, num_heads=8, embedding_dim=512, num_transformer_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2344b3-32fa-4f94-9c56-13b3ac96ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    model=model,\n",
    "    input_size=(1, 3, 224, 224),  # (batch_size, in_channels, img_size, img_size)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=15,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ece644-8389-406b-b8e9-802ac5a1f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_CLASSES = 7\n",
    "SEED = 123\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Define path\n",
    "SAVE_PATH = \"../Hasil Eksperimen\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Initialize lists to store training and validation metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Initialize the best metric for model saving\n",
    "best_val_accuracy = -float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{train_loss / (batch_idx + 1):.4f}\",\n",
    "            \"Acc\": f\"{correct / total:.4f}\"\n",
    "        })\n",
    "\n",
    "    # Calculate training accuracy and loss\n",
    "    train_accuracy = correct / total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Print training summary\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_targets = []\n",
    "    all_predicted = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        pbar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS} (Validation)\")\n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Update statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # Collect all targets and predictions for F1-score\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{val_loss / (batch_idx + 1):.4f}\",\n",
    "                \"Acc\": f\"{val_correct / val_total:.4f}\"\n",
    "            })\n",
    "\n",
    "    # Calculate validation accuracy, loss, and F1-score\n",
    "    val_accuracy = val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_f1 = f1_score(all_targets, all_predicted, average=\"weighted\")\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print validation summary\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_accuracy:.4f}, \"\n",
    "          f\"Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Step the learning rate scheduler based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Print the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        model_path = os.path.join(SAVE_PATH, \"coba2_best.pt\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Best model saved at {model_path} with val accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "    # Save loss and accuracy plots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker='o')\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(SAVE_PATH, \"coba2_loss.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Training Accuracy\", marker='o')\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label=\"Validation Accuracy\", marker='o')\n",
    "    plt.title(\"Accuracy per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    accuracy_plot_path = os.path.join(SAVE_PATH, \"coba2_acc.png\")\n",
    "    plt.savefig(accuracy_plot_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9419548-bc46-4909-a041-3c0173fec3c6",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3abd3f-0790-48ed-b3ec-17fe5434e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_model = ViT(num_classes=len(class_names), in_channels=3, patch_size=32, num_heads=8, embedding_dim=512, num_transformer_layers=12)\n",
    "best_model = best_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76e3b6-9f27-4e75-a83c-0734a22f310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state_dict\n",
    "model_path = \"../Hasil Eksperimen/coba2_best.pt\"\n",
    "best_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6a4ca-3dfb-4a39-9ea6-ecdca034e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "best_model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_targets = []\n",
    "all_predicted = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Update statistics\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += targets.size(0)\n",
    "        test_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Collect all targets and predictions\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{test_loss / (batch_idx + 1):.4f}\",\n",
    "            \"Acc\": f\"{test_correct / test_total:.4f}\"\n",
    "        })\n",
    "\n",
    "# Calculate test accuracy, loss, and F1-score\n",
    "test_accuracy = test_correct / test_total\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "conf_matrix = confusion_matrix(all_targets, all_predicted)\n",
    "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# Calculate classification report (includes precision, recall, F1-score, and support)\n",
    "class_report = classification_report(all_targets, all_predicted, target_names=[f\"Class {i}\" for i in range(NUM_CLASSES)])\n",
    "\n",
    "# Print test summary\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, \"\n",
    "      f\"Test Acc: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Class {i}: {acc:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79252d8-d103-4bf2-82ad-4098293a0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(image, mean, std):\n",
    "    \"\"\"\n",
    "    Denormalize a normalized image tensor.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Normalized image tensor (C, H, W).\n",
    "        mean (list): Mean used for normalization.\n",
    "        std (list): Standard deviation used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Denormalized image tensor.\n",
    "    \"\"\"\n",
    "    # Clone the image to avoid modifying the original\n",
    "    image = image.clone()\n",
    "    for c in range(image.shape[0]):\n",
    "        image[c] = image[c] * std[c] + mean[c]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1a78b-c049-45c7-8036-ab6cb3322a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "best_model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_targets = []\n",
    "all_predicted = []\n",
    "\n",
    "# Store misclassified images, true labels, and predicted labels\n",
    "misclassified_images = []\n",
    "misclassified_true = []\n",
    "misclassified_pred = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Update statistics\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += targets.size(0)\n",
    "        test_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Collect all targets and predictions\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Store misclassified images\n",
    "        misclassified_mask = predicted != targets\n",
    "        if misclassified_mask.any():\n",
    "            misclassified_images.extend(inputs[misclassified_mask].cpu())\n",
    "            misclassified_true.extend(targets[misclassified_mask].cpu().numpy())\n",
    "            misclassified_pred.extend(predicted[misclassified_mask].cpu().numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{test_loss / (batch_idx + 1):.4f}\",\n",
    "            \"Acc\": f\"{test_correct / test_total:.4f}\"\n",
    "        })\n",
    "\n",
    "# Calculate test accuracy, loss, and F1-score\n",
    "test_accuracy = test_correct / test_total\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "conf_matrix = confusion_matrix(all_targets, all_predicted)\n",
    "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# Calculate classification report (includes precision, recall, F1-score, and support)\n",
    "class_report = classification_report(all_targets, all_predicted, target_names=[f\"Class {i}\" for i in range(NUM_CLASSES)])\n",
    "\n",
    "# Print test summary\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, \"\n",
    "      f\"Test Acc: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Class {i}: {acc:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "def plot_misclassified_images(class_id, num_images=5, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"\n",
    "    Plots misclassified images for a specific class.\n",
    "\n",
    "    Args:\n",
    "        class_id (int): The class ID to visualize misclassifications for.\n",
    "        num_images (int): Number of misclassified images to display.\n",
    "        mean (list): Mean used for normalization.\n",
    "        std (list): Standard deviation used for normalization.\n",
    "    \"\"\"\n",
    "    # Filter misclassified images for the specified class\n",
    "    class_misclassified_indices = [i for i, true_label in enumerate(misclassified_true) if true_label == class_id]\n",
    "    class_misclassified_images = [misclassified_images[i] for i in class_misclassified_indices]\n",
    "    class_misclassified_true = [misclassified_true[i] for i in class_misclassified_indices]\n",
    "    class_misclassified_pred = [misclassified_pred[i] for i in class_misclassified_indices]\n",
    "\n",
    "    if not class_misclassified_images:\n",
    "        print(f\"No misclassified images found for Class {class_id}.\")\n",
    "        return\n",
    "\n",
    "    # Limit the number of images to display\n",
    "    num_images = min(num_images, len(class_misclassified_images))\n",
    "\n",
    "    # Plot the misclassified images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        img = class_misclassified_images[i]\n",
    "\n",
    "        # Denormalize the image\n",
    "        img = denormalize(img, mean, std)\n",
    "\n",
    "        # Convert from (C, H, W) to (H, W, C) and clip to valid range\n",
    "        img = img.permute(1, 2, 0)  # Change tensor shape for matplotlib\n",
    "        img = torch.clamp(img, 0, 1)  # Clip values to [0, 1]\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"True: {class_misclassified_true[i]}\\nPred: {class_misclassified_pred[i]}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(f\"Misclassified Images for Class {class_id}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c42eaa-e139-4ca5-ada7-6b81d34461a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "plot_misclassified_images(class_id=6, num_images=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a13bda-a439-47d2-aa33-8a5a5dbd36bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
