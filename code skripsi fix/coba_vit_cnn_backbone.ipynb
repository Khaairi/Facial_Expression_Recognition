{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f52c85-4ef4-4943-a521-ff740da659ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0+cu124'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85719059-ed85-4e86-a40a-e26217642235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b309cc-ab32-4ea3-8c91-f1821515f136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\MoKha\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torchvision.models import mobilenet_v3_large\n",
    "import random\n",
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77039c05-9f1a-430a-89b0-b22f0d4de2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe35ee-b631-403e-ae59-b7016ed13458",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bdddb0d-9e30-4147-938e-a8109e906f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "        # Ekstrak label dan piksel\n",
    "        self.labels = self.dataframe['emotion'].values\n",
    "        self.pixels = self.dataframe['pixels'].apply(self.string_to_image).values\n",
    "\n",
    "    def string_to_image(self, pixels_string):\n",
    "        # Konversi string piksel menjadi numpy array dan reshape ke 48x48\n",
    "        pixels = np.array(pixels_string.split(), dtype='float32')\n",
    "        image = pixels.reshape(48, 48)\n",
    "        image = np.expand_dims(image, axis=-1)  # Tambahkan channel dimensi\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixels[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.fromarray(image.squeeze().astype('uint8'), mode='L')\n",
    "\n",
    "        # Jika ada transformasi, terapkan ke image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd05f22-3a1d-495e-ab37-5912522b0f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train transforms: Compose(\n",
      "    Grayscale(num_output_channels=3)\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      ")\n",
      "test transforms: Compose(\n",
      "    Grayscale(num_output_channels=3)\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Create transform pipeline manually\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "]) \n",
    "\n",
    "# Create transform pipeline manually\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    " \n",
    "print(f\"train transforms: {train_transforms}\")\n",
    "print(f\"test transforms: {test_transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb51cb0-db73-45d5-b795-d7bbe982676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33325 entries, 0 to 33324\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   pixels   33325 non-null  object\n",
      " 1   emotion  33325 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 520.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('D://Kuliah//UPI//SEMESTER 8//dataset skripsi//fer2013_clean.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d525f87c-e3d3-4893-9194-b81b2a4b80d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 26992\n",
      "Validation set size: 3000\n",
      "Test set size: 3333\n"
     ]
    }
   ],
   "source": [
    "# Pertama, pisahkan data train (90%) dan validation (10%)\n",
    "data_train, data_test = train_test_split(data, test_size=0.1, stratify=data['emotion'], random_state=123)\n",
    "data_train, data_val = train_test_split(data_train, test_size=0.1, stratify=data_train['emotion'], random_state=123)\n",
    "\n",
    "# Cek ukuran masing-masing set untuk memastikan proporsi\n",
    "print(f'Train set size: {len(data_train)}')\n",
    "print(f'Validation set size: {len(data_val)}')\n",
    "print(f'Test set size: {len(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a112b864-08d8-42cb-a285-994f930f4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FERDataset(data_train, transform=train_transforms)\n",
    "val_dataset = FERDataset(data_val, transform=test_transforms)\n",
    "test_dataset = FERDataset(data_test, transform=test_transforms)\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d1c38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = data_train[\"emotion\"]\n",
    "# class_counts = np.bincount(train_labels)\n",
    "# class_weights = 1. / class_counts\n",
    "# sample_weights = class_weights[train_labels]\n",
    "\n",
    "# # Create a weighted sampler\n",
    "# sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27139b4d-f206-4e4f-82e6-6c26b311b7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the mean and std used for normalization\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "# Denormalize function\n",
    "def denormalize(tensor, mean, std):\n",
    "    # Clone the tensor to avoid modifying the original\n",
    "    tensor = tensor.clone()\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Reverse the normalization: (t * s) + m\n",
    "    return tensor\n",
    "\n",
    "# Ambil satu contoh dari train_dataset\n",
    "image, label = train_dataset[0]  # Index pertama dataset\n",
    "\n",
    "# Denormalize the image\n",
    "image = denormalize(image, mean, std)\n",
    "\n",
    "# Jika transform menghasilkan tensor, konversi ke format numpy\n",
    "if isinstance(image, torch.Tensor):\n",
    "    image = image.permute(1, 2, 0).numpy()  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "# Clip the values to the valid range [0, 1]\n",
    "image = image.clip(0, 1)\n",
    "\n",
    "# Plot gambar\n",
    "plt.imshow(image, cmap='gray')  # Gunakan cmap='gray' jika gambar grayscale\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')  # Hilangkan sumbu\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b8a0a00-f3ca-46e1-8080-faf95a66d2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_dataset[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f09b62-4109-49eb-ae60-3ce50168f049",
   "metadata": {},
   "source": [
    "## Build Vision Transformer (ViT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b7f5c19-e39f-478b-904a-4070f72d2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85ceb31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBackbone(nn.Module):\n",
    "    def __init__(self, in_channels=3, embedding_dim=768):\n",
    "        super().__init__()\n",
    "        # Output channels for each convolution layer\n",
    "        channels = [24, 48, 96, 192]\n",
    "        \n",
    "        # First conv: 3×3, stride 2, 3->24 channels\n",
    "        # Input: 224×224 -> Output: 112×112\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels[0], kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
    "        \n",
    "        # Second conv: 3×3, stride 2, 24->48 channels\n",
    "        # Input: 112×112 -> Output: 56×56\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels[1])\n",
    "        \n",
    "        # Third conv: 3×3, stride 2, 48->96 channels\n",
    "        # Input: 56×56 -> Output: 28×28\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(channels[2])\n",
    "        \n",
    "        # Fourth conv: 3×3, stride 2, 96->192 channels\n",
    "        # Input: 28×28 -> Output: 14×14\n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(channels[3])\n",
    "        \n",
    "        # Final 1×1 conv to match transformer dimension\n",
    "        self.final_conv = nn.Conv2d(channels[3], embedding_dim, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply convolutions with BN and ReLU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # Final 1×1 conv without activation\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abc468a7-138e-48ca-a495-a91e0988bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True)\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.multihead_attn(query=x,\n",
    "                                             key=x,\n",
    "                                             value=x,\n",
    "                                             need_weights=False)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76cdbef5-da91-45c7-8847-9de18bcd77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 mlp_size:int=3072,\n",
    "                 dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size,\n",
    "                      out_features=embedding_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cee8d7b1-8603-4ade-8bed-32444a8c942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 mlp_size:int=3072,\n",
    "                 mlp_dropout:float=0.,\n",
    "                 attn_dropout:float=0.):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        \n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.msa_block(self.layer_norm1(x)) + x \n",
    "        \n",
    "        x = self.mlp_block(self.layer_norm2(x)) + x \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfbe073e-b7c5-44c3-a123-d89a2152a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size:int=224,\n",
    "                 in_channels:int=3,\n",
    "                 num_transformer_layers:int=12,\n",
    "                 embedding_dim:int=768,\n",
    "                 mlp_size:int=3072,\n",
    "                 num_heads:int=12,\n",
    "                 attn_dropout:float=0.,\n",
    "                 mlp_dropout:float=0.,\n",
    "                 embedding_dropout:float=0.,\n",
    "                 num_classes:int=1000):\n",
    "        super().__init__()\n",
    "        self.cnn_backbone = CNNBackbone(in_channels, embedding_dim)\n",
    "        \n",
    "        self.seq_length = (img_size // 16) ** 2  # 14x14 = 196\n",
    "                 \n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "        \n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.seq_length + 1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "        self.norm = nn.LayerNorm(normalized_shape=embedding_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(in_features=embedding_dim, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "\n",
    "        # CNN backbone menghasilkan [B, 1, embedding_dim]\n",
    "        cnn_features = self.cnn_backbone(pixel_values)\n",
    "        \n",
    "        x = cnn_features.flatten(2).transpose(1, 2)  # [B, 196, embed_dim]\n",
    "        \n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) # (n_samples, 1, embed_dim)\n",
    "\n",
    "        x = torch.cat((class_token, x), dim=1) # (n_samples, 1 + n_patches, embed_dim)\n",
    "\n",
    "        x = self.position_embedding + x # add position embed\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        cls_token_final = x[:, 0]\n",
    "\n",
    "        logits = self.head(cls_token_final)\n",
    "    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccbb722-51b9-46af-90e2-4b80265eb878",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64b28361-dae6-43fa-b846-69caa47259a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTCNN(num_classes=len(class_names), in_channels=3, num_heads=8, embedding_dim=256, num_transformer_layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc2344b3-32fa-4f94-9c56-13b3ac96ee3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape     Output Shape    Param #         Trainable\n",
       "========================================================================================================================\n",
       "ViTCNN (ViTCNN)                                              [1, 3, 224, 224] [1, 7]          50,688          True\n",
       "├─CNNBackbone (cnn_backbone)                                 [1, 3, 224, 224] [1, 256, 14, 14] --              True\n",
       "│    └─Conv2d (conv1)                                        [1, 3, 224, 224] [1, 24, 112, 112] 672             True\n",
       "│    └─BatchNorm2d (bn1)                                     [1, 24, 112, 112] [1, 24, 112, 112] 48              True\n",
       "│    └─Conv2d (conv2)                                        [1, 24, 112, 112] [1, 48, 56, 56] 10,416          True\n",
       "│    └─BatchNorm2d (bn2)                                     [1, 48, 56, 56] [1, 48, 56, 56] 96              True\n",
       "│    └─Conv2d (conv3)                                        [1, 48, 56, 56] [1, 96, 28, 28] 41,568          True\n",
       "│    └─BatchNorm2d (bn3)                                     [1, 96, 28, 28] [1, 96, 28, 28] 192             True\n",
       "│    └─Conv2d (conv4)                                        [1, 96, 28, 28] [1, 192, 14, 14] 166,080         True\n",
       "│    └─BatchNorm2d (bn4)                                     [1, 192, 14, 14] [1, 192, 14, 14] 384             True\n",
       "│    └─Conv2d (final_conv)                                   [1, 192, 14, 14] [1, 256, 14, 14] 49,408          True\n",
       "├─Dropout (embedding_dropout)                                [1, 197, 256]   [1, 197, 256]   --              --\n",
       "├─Sequential (transformer_encoder)                           [1, 197, 256]   [1, 197, 256]   --              True\n",
       "│    └─TransformerEncoderBlock (0)                           [1, 197, 256]   [1, 197, 256]   --              True\n",
       "│    │    └─LayerNorm (layer_norm1)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [1, 197, 256]   [1, 197, 256]   263,168         True\n",
       "│    │    └─LayerNorm (layer_norm2)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MLPBlock (mlp_block)                             [1, 197, 256]   [1, 197, 256]   1,576,192       True\n",
       "│    └─TransformerEncoderBlock (1)                           [1, 197, 256]   [1, 197, 256]   --              True\n",
       "│    │    └─LayerNorm (layer_norm1)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [1, 197, 256]   [1, 197, 256]   263,168         True\n",
       "│    │    └─LayerNorm (layer_norm2)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MLPBlock (mlp_block)                             [1, 197, 256]   [1, 197, 256]   1,576,192       True\n",
       "│    └─TransformerEncoderBlock (2)                           [1, 197, 256]   [1, 197, 256]   --              True\n",
       "│    │    └─LayerNorm (layer_norm1)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [1, 197, 256]   [1, 197, 256]   263,168         True\n",
       "│    │    └─LayerNorm (layer_norm2)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MLPBlock (mlp_block)                             [1, 197, 256]   [1, 197, 256]   1,576,192       True\n",
       "│    └─TransformerEncoderBlock (3)                           [1, 197, 256]   [1, 197, 256]   --              True\n",
       "│    │    └─LayerNorm (layer_norm1)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [1, 197, 256]   [1, 197, 256]   263,168         True\n",
       "│    │    └─LayerNorm (layer_norm2)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MLPBlock (mlp_block)                             [1, 197, 256]   [1, 197, 256]   1,576,192       True\n",
       "│    └─TransformerEncoderBlock (4)                           [1, 197, 256]   [1, 197, 256]   --              True\n",
       "│    │    └─LayerNorm (layer_norm1)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [1, 197, 256]   [1, 197, 256]   263,168         True\n",
       "│    │    └─LayerNorm (layer_norm2)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MLPBlock (mlp_block)                             [1, 197, 256]   [1, 197, 256]   1,576,192       True\n",
       "│    └─TransformerEncoderBlock (5)                           [1, 197, 256]   [1, 197, 256]   --              True\n",
       "│    │    └─LayerNorm (layer_norm1)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [1, 197, 256]   [1, 197, 256]   263,168         True\n",
       "│    │    └─LayerNorm (layer_norm2)                          [1, 197, 256]   [1, 197, 256]   512             True\n",
       "│    │    └─MLPBlock (mlp_block)                             [1, 197, 256]   [1, 197, 256]   1,576,192       True\n",
       "├─LayerNorm (norm)                                           [1, 197, 256]   [1, 197, 256]   512             True\n",
       "├─Linear (head)                                              [1, 256]        [1, 7]          1,799           True\n",
       "========================================================================================================================\n",
       "Total params: 11,364,167\n",
       "Trainable params: 11,364,167\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 125.39\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 46.15\n",
       "Params size (MB): 38.94\n",
       "Estimated Total Size (MB): 85.69\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    model=model,\n",
    "    input_size=(1, 3, 224, 224),  # (batch_size, in_channels, img_size, img_size)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=15,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36ece644-8389-406b-b8e9-802ac5a1f946",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 100%|█████████████████████████████████████████| 422/422 [02:25<00:00,  2.89it/s, Loss=1.7781, Acc=0.2560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Train Loss: 1.7781, Train Acc: 0.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:06<00:00,  7.00it/s, Loss=1.7780, Acc=0.2677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Val Loss: 1.7780, Val Acc: 0.2677, Val F1: 0.2204\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\coba_vit_cnn_backbone_best.pt with val accuracy: 0.2677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: 100%|█████████████████████████████████████████| 422/422 [02:31<00:00,  2.78it/s, Loss=1.6722, Acc=0.3279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: Train Loss: 1.6722, Train Acc: 0.3279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:06<00:00,  6.74it/s, Loss=1.5983, Acc=0.3783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: Val Loss: 1.5983, Val Acc: 0.3783, Val F1: 0.3042\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\coba_vit_cnn_backbone_best.pt with val accuracy: 0.3783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: 100%|█████████████████████████████████████████| 422/422 [02:32<00:00,  2.77it/s, Loss=1.5458, Acc=0.3943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: Train Loss: 1.5458, Train Acc: 0.3943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:06<00:00,  6.85it/s, Loss=1.5181, Acc=0.4097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: Val Loss: 1.5181, Val Acc: 0.4097, Val F1: 0.3631\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\coba_vit_cnn_backbone_best.pt with val accuracy: 0.4097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: 100%|█████████████████████████████████████████| 422/422 [02:32<00:00,  2.77it/s, Loss=1.4602, Acc=0.4366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: Train Loss: 1.4602, Train Acc: 0.4366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:06<00:00,  6.89it/s, Loss=1.4679, Acc=0.4220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: Val Loss: 1.4679, Val Acc: 0.4220, Val F1: 0.3954\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\coba_vit_cnn_backbone_best.pt with val accuracy: 0.4220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: 100%|█████████████████████████████████████████| 422/422 [02:32<00:00,  2.76it/s, Loss=1.3945, Acc=0.4601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: Train Loss: 1.3945, Train Acc: 0.4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:06<00:00,  6.96it/s, Loss=1.4309, Acc=0.4403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: Val Loss: 1.4309, Val Acc: 0.4403, Val F1: 0.4008\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\coba_vit_cnn_backbone_best.pt with val accuracy: 0.4403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: 100%|█████████████████████████████████████████| 422/422 [02:32<00:00,  2.78it/s, Loss=1.3397, Acc=0.4809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: Train Loss: 1.3397, Train Acc: 0.4809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:06<00:00,  6.85it/s, Loss=1.3950, Acc=0.4517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: Val Loss: 1.3950, Val Acc: 0.4517, Val F1: 0.4283\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\coba_vit_cnn_backbone_best.pt with val accuracy: 0.4517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: 100%|█████████████████████████████████████████| 422/422 [02:32<00:00,  2.77it/s, Loss=1.2748, Acc=0.5106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: Train Loss: 1.2748, Train Acc: 0.5106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:06<00:00,  6.97it/s, Loss=1.4026, Acc=0.4600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: Val Loss: 1.4026, Val Acc: 0.4600, Val F1: 0.4561\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\coba_vit_cnn_backbone_best.pt with val accuracy: 0.4600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: 100%|█████████████████████████████████████████| 422/422 [02:31<00:00,  2.78it/s, Loss=1.2203, Acc=0.5309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: Train Loss: 1.2203, Train Acc: 0.5309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:06<00:00,  6.99it/s, Loss=1.3756, Acc=0.4680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: Val Loss: 1.3756, Val Acc: 0.4680, Val F1: 0.4648\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\coba_vit_cnn_backbone_best.pt with val accuracy: 0.4680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: 100%|█████████████████████████████████████████| 422/422 [02:31<00:00,  2.79it/s, Loss=1.1621, Acc=0.5548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: Train Loss: 1.1621, Train Acc: 0.5548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000 (Validation): 100%|██████████████████████████████| 47/47 [00:06<00:00,  7.04it/s, Loss=1.3584, Acc=0.4787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: Val Loss: 1.3584, Val Acc: 0.4787, Val F1: 0.4757\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\coba_vit_cnn_backbone_best.pt with val accuracy: 0.4787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: 100%|████████████████████████████████████████| 422/422 [02:31<00:00,  2.78it/s, Loss=1.1008, Acc=0.5794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: Train Loss: 1.1008, Train Acc: 0.5794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  7.01it/s, Loss=1.4255, Acc=0.4693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: Val Loss: 1.4255, Val Acc: 0.4693, Val F1: 0.4633\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: 100%|████████████████████████████████████████| 422/422 [02:32<00:00,  2.77it/s, Loss=1.0366, Acc=0.6076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: Train Loss: 1.0366, Train Acc: 0.6076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  6.88it/s, Loss=1.4332, Acc=0.4713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: Val Loss: 1.4332, Val Acc: 0.4713, Val F1: 0.4602\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: 100%|████████████████████████████████████████| 422/422 [02:31<00:00,  2.79it/s, Loss=0.9566, Acc=0.6392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: Train Loss: 0.9566, Train Acc: 0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  6.93it/s, Loss=1.4337, Acc=0.4713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: Val Loss: 1.4337, Val Acc: 0.4713, Val F1: 0.4651\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: 100%|████████████████████████████████████████| 422/422 [02:32<00:00,  2.77it/s, Loss=0.8813, Acc=0.6702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: Train Loss: 0.8813, Train Acc: 0.6702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  6.89it/s, Loss=1.4922, Acc=0.4860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: Val Loss: 1.4922, Val Acc: 0.4860, Val F1: 0.4766\n",
      "Current Learning Rate: 3e-05\n",
      "Best model saved at D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\\coba_vit_cnn_backbone_best.pt with val accuracy: 0.4860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: 100%|████████████████████████████████████████| 422/422 [02:32<00:00,  2.77it/s, Loss=0.8001, Acc=0.7015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: Train Loss: 0.8001, Train Acc: 0.7015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  6.81it/s, Loss=1.5162, Acc=0.4750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: Val Loss: 1.5162, Val Acc: 0.4750, Val F1: 0.4699\n",
      "Current Learning Rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: 100%|████████████████████████████████████████| 422/422 [02:33<00:00,  2.75it/s, Loss=0.7181, Acc=0.7323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: Train Loss: 0.7181, Train Acc: 0.7323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  6.91it/s, Loss=1.6841, Acc=0.4680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: Val Loss: 1.6841, Val Acc: 0.4680, Val F1: 0.4548\n",
      "Current Learning Rate: 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: 100%|████████████████████████████████████████| 422/422 [02:32<00:00,  2.77it/s, Loss=0.5007, Acc=0.8302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: Train Loss: 0.5007, Train Acc: 0.8302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  6.80it/s, Loss=1.7066, Acc=0.4750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: Val Loss: 1.7066, Val Acc: 0.4750, Val F1: 0.4734\n",
      "Current Learning Rate: 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: 100%|████████████████████████████████████████| 422/422 [02:33<00:00,  2.74it/s, Loss=0.4577, Acc=0.8463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: Train Loss: 0.4577, Train Acc: 0.8463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  6.88it/s, Loss=1.7383, Acc=0.4813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: Val Loss: 1.7383, Val Acc: 0.4813, Val F1: 0.4774\n",
      "Current Learning Rate: 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: 100%|████████████████████████████████████████| 422/422 [02:32<00:00,  2.76it/s, Loss=0.4271, Acc=0.8596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: Train Loss: 0.4271, Train Acc: 0.8596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  6.75it/s, Loss=1.7808, Acc=0.4780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: Val Loss: 1.7808, Val Acc: 0.4780, Val F1: 0.4751\n",
      "Current Learning Rate: 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: 100%|████████████████████████████████████████| 422/422 [02:31<00:00,  2.79it/s, Loss=0.4028, Acc=0.8691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: Train Loss: 0.4028, Train Acc: 0.8691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:07<00:00,  6.69it/s, Loss=1.8163, Acc=0.4797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: Val Loss: 1.8163, Val Acc: 0.4797, Val F1: 0.4751\n",
      "Current Learning Rate: 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: 100%|████████████████████████████████████████| 422/422 [02:34<00:00,  2.74it/s, Loss=0.3832, Acc=0.8760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: Train Loss: 0.3832, Train Acc: 0.8760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  6.83it/s, Loss=1.8766, Acc=0.4730]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: Val Loss: 1.8766, Val Acc: 0.4730, Val F1: 0.4691\n",
      "Current Learning Rate: 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: 100%|████████████████████████████████████████| 422/422 [02:32<00:00,  2.77it/s, Loss=0.3646, Acc=0.8824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: Train Loss: 0.3646, Train Acc: 0.8824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  6.85it/s, Loss=1.8953, Acc=0.4737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: Val Loss: 1.8953, Val Acc: 0.4737, Val F1: 0.4698\n",
      "Current Learning Rate: 3.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: 100%|████████████████████████████████████████| 422/422 [02:32<00:00,  2.76it/s, Loss=0.3326, Acc=0.8996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: Train Loss: 0.3326, Train Acc: 0.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:06<00:00,  6.81it/s, Loss=1.9074, Acc=0.4773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: Val Loss: 1.9074, Val Acc: 0.4773, Val F1: 0.4724\n",
      "Current Learning Rate: 3.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: 100%|████████████████████████████████████████| 422/422 [02:33<00:00,  2.76it/s, Loss=0.3278, Acc=0.9021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: Train Loss: 0.3278, Train Acc: 0.9021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000 (Validation): 100%|█████████████████████████████| 47/47 [00:07<00:00,  6.47it/s, Loss=1.9088, Acc=0.4763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: Val Loss: 1.9088, Val Acc: 0.4763, Val F1: 0.4723\n",
      "Current Learning Rate: 3.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000:   1%|▍                                         | 5/422 [00:01<02:38,  2.62it/s, Loss=0.3471, Acc=0.9062]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m     38\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m     40\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE), targets\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mFERDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Jika ada transformasi, terapkan ke image\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 28\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv_(std)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_CLASSES = 7\n",
    "SEED = 123\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, generator=torch.Generator().manual_seed(SEED))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Define path\n",
    "SAVE_PATH = \"D://Kuliah//UPI//SEMESTER 8//hasil eksperimen\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Initialize lists to store training and validation metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Initialize the best metric for model saving\n",
    "best_val_accuracy = -float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{train_loss / (batch_idx + 1):.4f}\",\n",
    "            \"Acc\": f\"{correct / total:.4f}\"\n",
    "        })\n",
    "\n",
    "    # Calculate training accuracy and loss\n",
    "    train_accuracy = correct / total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Print training summary\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_targets = []\n",
    "    all_predicted = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        pbar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS} (Validation)\")\n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Update statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # Collect all targets and predictions for F1-score\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{val_loss / (batch_idx + 1):.4f}\",\n",
    "                \"Acc\": f\"{val_correct / val_total:.4f}\"\n",
    "            })\n",
    "\n",
    "    # Calculate validation accuracy, loss, and F1-score\n",
    "    val_accuracy = val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_f1 = f1_score(all_targets, all_predicted, average=\"weighted\")\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print validation summary\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_accuracy:.4f}, \"\n",
    "          f\"Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Step the learning rate scheduler based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Print the current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        model_path = os.path.join(SAVE_PATH, \"coba_vit_cnn_backbone_best.pt\")\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "        }, model_path)\n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Best model saved at {model_path} with val accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "    # Save loss and accuracy plots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker='o')\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(SAVE_PATH, \"coba_vit_cnn_backbone_loss.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Training Accuracy\", marker='o')\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label=\"Validation Accuracy\", marker='o')\n",
    "    plt.title(\"Accuracy per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    accuracy_plot_path = os.path.join(SAVE_PATH, \"coba_vit_cnn_backbone_accuracy.png\")\n",
    "    plt.savefig(accuracy_plot_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9419548-bc46-4909-a041-3c0173fec3c6",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca0091df",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_model = ViTCNN(num_classes=len(class_names), in_channels=3, num_heads=8, embedding_dim=256, num_transformer_layers=6)\n",
    "best_model = best_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb98608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoKha\\AppData\\Local\\Temp\\ipykernel_26124\\1139574884.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"D://Kuliah//UPI//SEMESTER 8//hasil eksperimen//coba_vit_cnn_backbone_best.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"D://Kuliah//UPI//SEMESTER 8//hasil eksperimen//coba_vit_cnn_backbone_best.pt\")\n",
    "best_model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91d6a4ca-3dfb-4a39-9ea6-ecdca034e4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████| 53/53 [00:08<00:00,  6.45it/s, Loss=1.4984, Acc=0.4791]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.4984, Test Acc: 0.4791\n",
      "\n",
      "Per-Class Accuracy:\n",
      "Class 0: 0.3089\n",
      "Class 1: 0.0222\n",
      "Class 2: 0.2712\n",
      "Class 3: 0.7154\n",
      "Class 4: 0.3444\n",
      "Class 5: 0.6226\n",
      "Class 6: 0.5193\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.42      0.31      0.36       463\n",
      "     Class 1       1.00      0.02      0.04        45\n",
      "     Class 2       0.34      0.27      0.30       472\n",
      "     Class 3       0.68      0.72      0.70       868\n",
      "     Class 4       0.35      0.34      0.35       572\n",
      "     Class 5       0.62      0.62      0.62       318\n",
      "     Class 6       0.38      0.52      0.44       595\n",
      "\n",
      "    accuracy                           0.48      3333\n",
      "   macro avg       0.54      0.40      0.40      3333\n",
      "weighted avg       0.48      0.48      0.47      3333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "best_model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_targets = []\n",
    "all_predicted = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Update statistics\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += targets.size(0)\n",
    "        test_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Collect all targets and predictions\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{test_loss / (batch_idx + 1):.4f}\",\n",
    "            \"Acc\": f\"{test_correct / test_total:.4f}\"\n",
    "        })\n",
    "\n",
    "# Calculate test accuracy, loss, and F1-score\n",
    "test_accuracy = test_correct / test_total\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "conf_matrix = confusion_matrix(all_targets, all_predicted)\n",
    "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# Calculate classification report (includes precision, recall, F1-score, and support)\n",
    "class_report = classification_report(all_targets, all_predicted, target_names=[f\"Class {i}\" for i in range(NUM_CLASSES)])\n",
    "\n",
    "# Print test summary\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, \"\n",
    "      f\"Test Acc: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Class {i}: {acc:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79252d8-d103-4bf2-82ad-4098293a0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(image, mean, std):\n",
    "    \"\"\"\n",
    "    Denormalize a normalized image tensor.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Normalized image tensor (C, H, W).\n",
    "        mean (list): Mean used for normalization.\n",
    "        std (list): Standard deviation used for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Denormalized image tensor.\n",
    "    \"\"\"\n",
    "    # Clone the image to avoid modifying the original\n",
    "    image = image.clone()\n",
    "    for c in range(image.shape[0]):\n",
    "        image[c] = image[c] * std[c] + mean[c]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1a78b-c049-45c7-8036-ab6cb3322a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "best_model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_targets = []\n",
    "all_predicted = []\n",
    "\n",
    "# Store misclassified images, true labels, and predicted labels\n",
    "misclassified_images = []\n",
    "misclassified_true = []\n",
    "misclassified_pred = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Update statistics\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += targets.size(0)\n",
    "        test_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Collect all targets and predictions\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Store misclassified images\n",
    "        misclassified_mask = predicted != targets\n",
    "        if misclassified_mask.any():\n",
    "            misclassified_images.extend(inputs[misclassified_mask].cpu())\n",
    "            misclassified_true.extend(targets[misclassified_mask].cpu().numpy())\n",
    "            misclassified_pred.extend(predicted[misclassified_mask].cpu().numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{test_loss / (batch_idx + 1):.4f}\",\n",
    "            \"Acc\": f\"{test_correct / test_total:.4f}\"\n",
    "        })\n",
    "\n",
    "# Calculate test accuracy, loss, and F1-score\n",
    "test_accuracy = test_correct / test_total\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "conf_matrix = confusion_matrix(all_targets, all_predicted)\n",
    "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# Calculate classification report (includes precision, recall, F1-score, and support)\n",
    "class_report = classification_report(all_targets, all_predicted, target_names=[f\"Class {i}\" for i in range(NUM_CLASSES)])\n",
    "\n",
    "# Print test summary\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, \"\n",
    "      f\"Test Acc: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Class {i}: {acc:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "def plot_misclassified_images(class_id, num_images=5, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"\n",
    "    Plots misclassified images for a specific class.\n",
    "\n",
    "    Args:\n",
    "        class_id (int): The class ID to visualize misclassifications for.\n",
    "        num_images (int): Number of misclassified images to display.\n",
    "        mean (list): Mean used for normalization.\n",
    "        std (list): Standard deviation used for normalization.\n",
    "    \"\"\"\n",
    "    # Filter misclassified images for the specified class\n",
    "    class_misclassified_indices = [i for i, true_label in enumerate(misclassified_true) if true_label == class_id]\n",
    "    class_misclassified_images = [misclassified_images[i] for i in class_misclassified_indices]\n",
    "    class_misclassified_true = [misclassified_true[i] for i in class_misclassified_indices]\n",
    "    class_misclassified_pred = [misclassified_pred[i] for i in class_misclassified_indices]\n",
    "\n",
    "    if not class_misclassified_images:\n",
    "        print(f\"No misclassified images found for Class {class_id}.\")\n",
    "        return\n",
    "\n",
    "    # Limit the number of images to display\n",
    "    num_images = min(num_images, len(class_misclassified_images))\n",
    "\n",
    "    # Plot the misclassified images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        img = class_misclassified_images[i]\n",
    "\n",
    "        # Denormalize the image\n",
    "        img = denormalize(img, mean, std)\n",
    "\n",
    "        # Convert from (C, H, W) to (H, W, C) and clip to valid range\n",
    "        img = img.permute(1, 2, 0)  # Change tensor shape for matplotlib\n",
    "        img = torch.clamp(img, 0, 1)  # Clip values to [0, 1]\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"True: {class_misclassified_true[i]}\\nPred: {class_misclassified_pred[i]}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(f\"Misclassified Images for Class {class_id}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c42eaa-e139-4ca5-ada7-6b81d34461a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "plot_misclassified_images(class_id=6, num_images=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a13bda-a439-47d2-aa33-8a5a5dbd36bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
